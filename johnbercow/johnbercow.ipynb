{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'domesticator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mBio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SeqIO, PDB, SeqUtils, Seq\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdomesticator\u001b[39;00m \u001b[38;5;66;03m# Ryan's domesticator with minor modifications.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01midt\u001b[39;00m \u001b[38;5;66;03m# IDT API - from Ryan.\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'domesticator'"
     ]
    }
   ],
   "source": [
    "#!/software/containers/john_bercow.sif\n",
    "\n",
    "# Generates an IDT-ready .xlsx file for ordering eBlocks from a folder of PDBs and/or a FASTA file.\n",
    "# Compatible overhangs for Golden Gate cloning are added automatically.\n",
    "# Reverse translation is performed with Ryan's Domesticator.\n",
    "# Sequences are queried against IDT (courtesy of Ryan) to ensure synthesiability.\n",
    "# RECOMMENDED: check your GG assemblies at https://goldengate.neb.com/#!/\n",
    "# Wondering why the script is called John Bercow? https://www.youtube.com/watch?v=VYycQTm2HrM&ab_channel=TheSun\n",
    "\n",
    "# ============================================\n",
    "# TODO\n",
    "# ============================================\n",
    "# Splitting beyond 2 fragments\n",
    "# Automatic layout for fragmented hetero-oligomers\n",
    "\n",
    "# ============================================\n",
    "# LIBRARIES\n",
    "# ============================================\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import argparse\n",
    "import datetime; date = datetime.datetime.now().strftime('%Y_%m_%d')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Bio import SeqIO, PDB, SeqUtils, Seq\n",
    "import domesticator # Ryan's domesticator with minor modifications.\n",
    "import idt # IDT API - from Ryan.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get/make IDT API credentials to query sequence complexity.\n",
    "idt.user_info_file, idt.token_file = idt.use_dir(\"~/idt_credentials\")\n",
    "idt_user_info = idt.get_user_info(idt.user_info_file)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# HARD-CODED DEFINITIONS\n",
    "# ============================================\n",
    "# Restriction enzyme and Cterm/Nterm protein tags of the different GG vectors.\n",
    "# Enzyme / Nterm-tag / Cterm-tag / 5'-sticky / 3'-sticky / description\n",
    "vectors = {\n",
    "    'LM0627':['BsaI','MSG','GSGSHHWGSTHHHHHH','agga','ggttcc', 'C-term SNAC-His'],\n",
    "    'LM0668':['BsaI','MGLPDSLEFIASKLAWHHHHHHSG','GSGSSGSGEGQQHHLGGAKQAGDV','agga','ggttcc', 'N-term MGLP and C-term GS-FGG'],\n",
    "    'LM0670':['BsaI','MSG','GSHHHHHH','agga','ggttcc', 'C-term His'],\n",
    "    'LM0671':['BsaI','MSKGEELFTGVVPILVELDGDVNGHKFSVRGEGEGDATNGKLTLKFICTTGKLPVPWPTLVTTLTYGVQCFARYPDHMKQHDFFKSAMPEGYVQERTISFKDDGTYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNFNSHNVYITADKQKNGIKANFKIRHNVEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSVLSKDPNEKRDHMVLLEFVTAAGITHGMDELYKGSSG','GSHHHHHH','agga','ggttcc', 'N-term sfGFP and C-term His'],\n",
    "    'LM0673':['BsaI','MSKGEELFTGVVPILVELDGDVNGHKFSVRGEGEGDATNGKLTLKFICTTGKLPVPWPTLVTTLTYGVQCFARYPDHMKQHDFFKSAMPEGYVQERTISFKDDGTYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNFNSHNVYITADKQKNGIKANFKIRHNVEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSVLSKDPNEKRDHMVLLEFVTAAGITHGMDELYKGGSHHWSSG','GSHHHHHH','agga','ggttcc', 'N-term sfGFP-SNAC and C-term His'],\n",
    "    'LM1371':['BsaI','MSHHHHHHSG','GS','agga','ggttcc', 'N-term His'],\n",
    "    'LM1425':['BsaI','MSG','GSGLNDIFEAQKIEWHESHHHHHH','agga','ggttcc', 'C-term AviTag-His'],\n",
    "    'MA0001':['SapI','MGK','','aaa','tga', 'pDecoy (mammalian vector)'],\n",
    "    'MA0002':['SapI','MKLLSSIEQACDICRLKKLKCSKEKPKCAKCLKNNWECRYSPKTKRSPLTRAHLTEVESRLERLEQLFLLIFPREDLDMILKMDSLQDIKALLTGLFVQDNVNKDAVTDRLASVETDMPLTLRQHRISATSSSEESSNKGQRQLTVSPEFPGGSK','','aaa','tga', 'pBind-GAL4 (mammalian vector)'],\n",
    "    'MA0003':['SapI','MKLLSSIEQACPKKKRKVDEFPGISTAPPTDVSLGDELHLDGEDVAMAHADALDDFDLDMLGDGDSPGPGSPGGSK','','aaa','tga', 'pAct-VP16 (mammalian vector)'],\n",
    "    'MA0004':['SapI','MKLLSSIEQACPKKKRKVDEFPGISTAPPTDVSLGDELHLDGEDVAMAHADALDDFDLDMLGDGDSPGPGSPEAAAK','','aaa','tga', 'pAct-rigid-VP16 (mammalian vector)'],\n",
    "    'MA0005':['SapI','xxx','xxx','aaa','tga', 'pBind-ZF6-4 (mammalian vector)'],\n",
    "    'MA0006':['SapI','xxx','xxx','aaa','tga', 'pAct-p65 (mammalian vector)'],\n",
    "    'MA0007':['BsaI','xxx','xxx','agga','ggttcc', 'N-term LHD101A and C-term SNAC-His'],\n",
    "    'MA0008':['BsaI','xxx','xxx','agga','ggttcc', 'N-term LHD101B and C-term SNAC-His'],\n",
    "    'MA0009':['SapI','xxx','xxx','aaa','tga', 'soluble mammalian expression'],\n",
    "    'MA0010':['SapI','xxx','xxx','aaa','tga', 'thethered mammalian expression'],\n",
    "    'BW1001':['BsaI','MSG','GSSGSGGSGGGGSGGSSSGGVTGYRLFEEILGSHHHHHH','agga','ggttcc', 'C-term smBiT-His'],\n",
    "    'BW1002':['BsaI','MSG','GSSGSGGSGGGGSGGSSSGGVTGYRLFEEILGSTYKLILNGKTLKGETTTEAVDAATAEKVFKQYANDNGVDGEWTYDDATKTFTVTEGSHHHHHH','agga','ggttcc', 'C-term smBiT-GB1-His'],\n",
    "    'BW1003':['BsaI','MSG','GSSGSGGSGGGGSGGSSSGGVFTLEDFVGDWEQTAAYNLDQVLEQGGVSSLLQNLAVSVTPIQRIVRSGENALKIDIHVIIPYEGLSADQMAQIEEVFKVVYPVDDHHFKVILPYGTLVIDGVTPNMLNYFGRPYEGIAVFDGKKITVTGTLWNGNKIIDERLITPDGSMLFRVTINSGSHHHHHH','agga','ggttcc', 'C-term lgBiT'],\n",
    "    'BW1004':['BsaI','MSGHHHHHHGSVTGYRLFEEILGGSGSGGSGGGGSGGSSSGG','GS','agga','ggttcc', 'N-term His-smBiT'],\n",
    "    'BW1005':['BsaI','MSGHHHHHHGSTYKLILNGKTLKGETTTEAVDAATAEKVFKQYANDNGVDGEWTYDDATKTFTVTEGSVTGYRLFEEILGGSGSGGSGGGGSGGSSSGG','GS','agga','ggttcc', 'N-term His-GB1-smBiT'],\n",
    "    'BW1006':['BsaI','MSGHHHHHHGSVFTLEDFVGDWEQTAAYNLDQVLEQGGVSSLLQNLAVSVTPIQRIVRSGENALKIDIHVIIPYEGLSADQMAQIEEVFKVVYPVDDHHFKVILPYGTLVIDGVTPNMLNYFGRPYEGIAVFDGKKITVTGTLWNGNKIIDERLITPDGSMLFRVTINSGGSGSGGSGGGGSGGSSSGG','GS','agga','ggttcc', 'N-term His-lgBiT'],\n",
    "    'AL0001':['BsaI','MSG','GS','agga','ggttcc', 'Just your POI (almost)'],\n",
    "    'PL1337':['BsaI','MSGYKLILNGKTLKGETTTEAVDAATAEKVFKQYANDNGVDGEWTYDDATKTFTVTEGSSG','GSHHHHHH','agga','ggttcc', 'N-term GB1 and C-term His'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7db65509610b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvec_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf' - {k} ({v[0]}): {v[5]} | {v[1]}[...]{v[2]}'\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# for argparse description\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# All lab-available plasmids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgg_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/software/lab/johnbercow/entry_vectors/*.fa'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mentry_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vectors' is not defined"
     ]
    }
   ],
   "source": [
    "vec_str = '\\n'.join([f' - {k} ({v[0]}): {v[5]} | {v[1]}[...]{v[2]}' for k, v in vectors.items()]) # for argparse description\n",
    "\n",
    "# All lab-available plasmids\n",
    "gg_vectors = glob.glob('/software/lab/johnbercow/entry_vectors/*.fa')\n",
    "entry_vectors = {}\n",
    "for v in gg_vectors:\n",
    "    records = SeqIO.parse(v, 'fasta')\n",
    "    for record in records:\n",
    "        entry_vectors[record.id.split('_')[0]] = str(record.seq.lower())\n",
    "\n",
    "# Type IIS restriction enzymes cut sites. Both the  forward and reverse complement sequences (each 5'->3') are indicated.\n",
    "avoid_seq = {\n",
    "    'BsaI':['GGTCTC', 'GAGACC'],\n",
    "    'SapI':['GCTCTTC', 'GAAGAGC']\n",
    "}\n",
    "\n",
    "# GG adapters.\n",
    "gg_adapters = {\n",
    "    'BsaI':{\n",
    "        '5prime':'atactacggtctca',\n",
    "        '3prime':'cgagaccgtaatgc',\n",
    "        },\n",
    "    'SapI':{\n",
    "        '5prime':'atactacgctcttcg',\n",
    "        '3prime':'cgaagagcgtaatgc',\n",
    "        }\n",
    "}\n",
    "\n",
    "# FW, RV, N_spacer, N_sticky -- for generating the plasmid maps\n",
    "cuts = {\n",
    "    'BsaI':['ggtctc', 'gagacc', 1, 4],\n",
    "    'SapI':['gctcttc', 'gaagagc', 1, 3]\n",
    "}\n",
    "\n",
    "base_pairing = {'C':'G', 'G':'C', 'A':'T', 'T':'A'}\n",
    "# Pool of 4bp sticky ends. Used for spliting genes into 2 fragments if necessary. ONLY FOR LM VECTORS!!!\n",
    "# Overhangs (4 bp) with high T4 ligase fidelity. From Ligase Fidelity Viewer (v2) https://ggtools.neb.com/viewset/run.cgi\n",
    "sticky_4bp = ['AAGG', 'ACTC', 'AGGA', 'ATCA', 'GCCG', 'CTGA', 'GCGA', 'GGAA', 'GTTT']\n",
    "sticky_4bp_comp = [''.join([base_pairing[b] for b in st]) for st in sticky_4bp] # complementary sequences\n",
    "sticky_4bp_revcomp = [s[::-1] for s in sticky_4bp_comp] # reverse complementary sequences\n",
    "sticky_4bp = sticky_4bp + sticky_4bp_revcomp # by convention, consider 5'->3' sequences only.\n",
    "sticky_4bp_already_used = ['AGGA', 'TTCC', 'TCCT', 'GGAA'] # remove these sites from the pool\n",
    "sticky_4bp = list(set([st for st in sticky_4bp if st not in sticky_4bp_already_used]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vec_str' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-035564f3071a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m parser = argparse.ArgumentParser(\n\u001b[1;32m      5\u001b[0m         \u001b[0mformatter_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRawTextHelpFormatter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\" * Generates an IDT-ready .xlsx file for ordering eBlocks from a folder of PDBs and/or a concatenated FASTA file.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m                     \u001b[0;34m\" * Appropriate overhangs for Golden Gate cloning into entry vector(s) of interest are added automatically.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                     \u001b[0;34m\" * Reverse translation is performed with Ryan's Domesticator.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vec_str' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# ARGUMENTS\n",
    "# ============================================\n",
    "parser = argparse.ArgumentParser(\n",
    "        formatter_class=argparse.RawTextHelpFormatter,\n",
    "        description=\" * Generates an IDT-ready .xlsx file for ordering eBlocks from a folder of PDBs and/or a concatenated FASTA file.\\n\"\n",
    "                    \" * Appropriate overhangs for Golden Gate cloning into entry vector(s) of interest are added automatically.\\n\"\n",
    "                    \" * Reverse translation is performed with Ryan's Domesticator.\\n\"\n",
    "                    \" * Sequences are queried against IDT (courtesy of Ryan), and RT is repeated until synthesiability is achieved.\\n\"\n",
    "                    \" * RECOMMENDED: check your GG assemblies at https://goldengate.neb.com/#!/\\n\"\n",
    "                    \" * Wondering why the script is called John Bercow? https://www.youtube.com/watch?v=VYycQTm2HrM&ab_channel=TheSun\\n\"\n",
    "                    \"\\n\"\n",
    "                    \" * AVAILABLE ENTRY VECTORS:\\n\"\n",
    "                    \" *** see /net/software/lab/johnbercow/entry_vectors/ for the FULL list ***\\n\"\n",
    "                   f\"{vec_str}\\n\"\n",
    "        )\n",
    "# REQUIRED\n",
    "parser.add_argument(\n",
    "        '--order_pdbs',\n",
    "        help='path to a folder containing the PDBs you want to order. Either this option or --order_fasta needs to be specified. Both can specified.',\n",
    "        action='store',\n",
    "        type=str\n",
    "        )\n",
    "parser.add_argument(\n",
    "        '--order_fasta',\n",
    "        help='path to a FASTA file containing the AA sequences you want to order. Either this option or --order_pdbs needs to be specified. Both can be specified.',\n",
    "        action='store',\n",
    "        type=str\n",
    "        )\n",
    "parser.add_argument(\n",
    "        '--order_name',\n",
    "        help='name of the order (appended to output files). NB date, plate IDs, organism, and enzyme are added automatically.',\n",
    "        action='store',\n",
    "        type=str,\n",
    "        required=True\n",
    "        )\n",
    "parser.add_argument(\n",
    "        '--gg_vector',\n",
    "        help='name of target vector(s) for Golden Gate cloning (determines the DNA adapters). Also determines the AA tags appended to the design in the FASTA output. Multiple vectors can be specified as space-separated values, but they need to be compatible (i.e. use the same enzyme and same overhangs).',\n",
    "        action='store',\n",
    "        required=True,\n",
    "#        choices=vectors.keys(),\n",
    "        nargs='+'\n",
    "        )\n",
    "parser.add_argument(\n",
    "        '--species',\n",
    "        help='codon optimisation will be performed for this species (e.g. e_coli, s_cerevisiae, h_sapiens, etc...)',\n",
    "        action='store',\n",
    "        type=str,\n",
    "        required=True\n",
    "        )\n",
    "parser.add_argument(\n",
    "        '--design_prefix',\n",
    "        help='designs get IDs with this prefix (e.g. LM0001, LM0002, etc...)',\n",
    "        action='store',\n",
    "        type=str,\n",
    "        required=True\n",
    "        )\n",
    "parser.add_argument(\n",
    "        '--design_id',\n",
    "        help='increment design indices from this number.',\n",
    "        action='store',\n",
    "        type=int,\n",
    "        required=True\n",
    "        )\n",
    "\n",
    "# OPTIONAL\n",
    "parser.add_argument(\n",
    "        '--skip_idt_query',\n",
    "        help=\"skip IDT website query that checks synthesisability of eBlocks.\",\n",
    "        action='store_true',\n",
    "        )\n",
    "parser.add_argument(\n",
    "        '--idt_score',\n",
    "        help=\"IDT complexity score threshold for accepting a reverse translated sequence (Default: 7). 0-7 is green, 7-15 is yellow, >15 is red and IDT will not make it.\",\n",
    "        action='store',\n",
    "        type=float,\n",
    "        default=7\n",
    "        )\n",
    "parser.add_argument(\n",
    "        '--starting_kmers_weight',\n",
    "        help=\"starting value for the kmers_weight setting of Domesticator (Default: 10). This parameter is linearly ramped (up to 100) over --n_domesticator_steps.\",\n",
    "        action='store',\n",
    "        type=int,\n",
    "        default=10\n",
    "        )\n",
    "parser.add_argument(\n",
    "        '--n_domesticator_steps',\n",
    "        help=\"maximum number of Domesticator steps attempted (Default: 10). The kmers_weight parameter (which increases synthesiability of repetitive sequences) is linearly ramped up to 100 over this number of steps.\",\n",
    "        action='store',\n",
    "        type=int,\n",
    "        default=10\n",
    "        )\n",
    "parser.add_argument(\n",
    "        '--max_attempts',\n",
    "        help=\"maximum number of reverse translation attempts at each Domesticator step (Default: 20). Since Domesticator is stochastic, re-running the optimisation problem with the same parameters can lead to different solutions.\",\n",
    "        action='store',\n",
    "        type=int,\n",
    "        default=20\n",
    "        )\n",
    "parser.add_argument(\n",
    "        '--max_length',\n",
    "        help=\"maximum length of eBlocks. IDT's maximum is 1500 bp, but less can be specified if sequence complexity is a issue for synthesis and you want to force the generation of smaller fragments.\",\n",
    "        action='store',\n",
    "        type=int,\n",
    "        default=1500\n",
    "        )\n",
    "parser.add_argument(\n",
    "        '--print_hto',\n",
    "        help=\"print heterooligomers to stdout.\",\n",
    "        action='store_true',\n",
    "        )\n",
    "parser.add_argument(\n",
    "        '--no_layout',\n",
    "        help=\"do not apply automated layout formatting.\",\n",
    "        action='store_true',\n",
    "        )\n",
    "parser.add_argument(\n",
    "        '--no_plasmids',\n",
    "        help=\"do not generate the cloned plasmid maps.\",\n",
    "        action='store_true',\n",
    "        )\n",
    "parser.add_argument(\n",
    "        '--verbose',\n",
    "        help=\"increase the verbosity of th e output (recommended).\",\n",
    "        action='store_true',\n",
    "        )\n",
    "parser.add_argument(\n",
    "        '--echo',\n",
    "        help=\"generates outputs formated as 384w plates (for ordering into ECHO-qualified plates). Currently only available in conjuction with the no_layout option.\",\n",
    "        action='store_true',\n",
    "        )\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Sanity checks.\n",
    "if args.order_pdbs == None and args.order_fasta == None:\n",
    "    print('Either a folder containing PDBs or a FASTA file need to be specified. System exiting...')\n",
    "    sys.exit()\n",
    "\n",
    "if args.order_pdbs == None:\n",
    "\n",
    "    if '/' in args.order_fasta:\n",
    "        output_folder = '/'.join(args.order_fasta.split('/')[:-1]) + '/'\n",
    "\n",
    "    else:\n",
    "        output_folder = './'\n",
    "\n",
    "else:\n",
    "    output_folder = args.order_pdbs\n",
    "\n",
    "if args.max_length < 600:\n",
    "    print('IDT cannot synthesise eBlocks outside of the 300-1500 bp range. Splitting a gene of less than 600 bp will generate fragments that are smaller than 300 bp each. System exiting...')\n",
    "    sys.exit()\n",
    "\n",
    "if args.idt_score > 10.0:\n",
    "    print('IDT will not synthesise anything with a complexity score above 10. System exiting...')\n",
    "    sys.exit()\n",
    "\n",
    "try:\n",
    "    enzyme = np.unique([vectors[gg_v][0] for gg_v in args.gg_vector])\n",
    "    if len(enzyme) > 1:\n",
    "        print(f'The destination vectors ({\", \".join(args.gg_vector)}) are not compatible. System exiting...')\n",
    "        sys.exit()\n",
    "\n",
    "    else:\n",
    "        enzyme = enzyme[0]\n",
    "        sticky5prime, sticky3prime = vectors[args.gg_vector[0]][3], vectors[args.gg_vector[0]][4]\n",
    "\n",
    "except:\n",
    "    enzyme = 'BsaI'\n",
    "    sticky5prime, sticky3prime = vectors['LM0627'][3], vectors['LM0627'][4]\n",
    "    print('Assuming LM0627-like overhangs for the selected vector(s).')\n",
    "    print('!!! DO NOT ORDER IF THIS IS NOT THE CASE AS YOU WON\\'T BE ABLE TO CLONE YOUR EBLOCKS !!!')\n",
    "\n",
    "\n",
    "if args.echo and not args.no_layout:\n",
    "    print('The --echo  option is currently only available in conjuction with --no_layout. Switching to this...')\n",
    "    args.no_layout = True\n",
    "\n",
    "filename = f'{output_folder}{date}_{args.order_name}_{args.species}_{enzyme}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FUNCTIONS\n",
    "# ============================================\n",
    "\n",
    "def adjust_for_eblock(dna_seq, max_length, gg_int_adapters, avoid_seqs):\n",
    "    '''\n",
    "    Check size of DNA sequences and either pad it (if too short), or split it (if too long).\n",
    "    '''\n",
    "    # Check if the sequence fits the the size limits\n",
    "    if len(dna_seq) <= max_length:\n",
    "\n",
    "        if len(dna_seq) < 300:\n",
    "            print(f'  [!] DNA sequence is too short to be ordered as an eBlock ({len(dna_seq)} vs. 300 bp). Adding some padding to the sequence...')\n",
    "            pad_length = ((300 - (len(dna_seq))) // 2 )\n",
    "            extra = 300 - len(dna_seq) - (2 * pad_length)\n",
    "            if extra < 0 :\n",
    "                extra = 0\n",
    "\n",
    "            # Pad sequence\n",
    "            pad_nocut = False\n",
    "            while pad_nocut == False:\n",
    "\n",
    "                pad5prime = ''.join(np.random.choice(['A','T','C','G'], size=pad_length + extra))\n",
    "                pad3prime = ''.join(np.random.choice(['A','T','C','G'], size=pad_length))\n",
    "\n",
    "                for a_seq in avoid_seqs:\n",
    "\n",
    "                    if (a_seq in pad5prime) or (a_seq in pad3prime):\n",
    "                        pad_nocut = False\n",
    "\n",
    "                    else:\n",
    "                        pad_nocut = True\n",
    "\n",
    "            dna_seq =  pad5prime + dna_seq + pad3prime\n",
    "            dna_fragments = {'':dna_seq}\n",
    "\n",
    "        else:\n",
    "            dna_fragments = {'':dna_seq}\n",
    "\n",
    "    else:\n",
    "        if len(dna_seq) > (2 * args.max_length - ((14+4)*2)):\n",
    "            print(f'  [!] Lengths of the fragments after 2-way splitting will be too long for eBlock synthesis (>{int(len(dna_seq)/2)} bp each). Splitting into more than 2 fragments is currently not supported.')\n",
    "            print(f'  [!] This design cannot currently be ordered as eBlocks. Ignoring it and moving on...')\n",
    "            print(f'  ####################################################################################')\n",
    "            dna_fragments = {'':None}\n",
    "\n",
    "        else:\n",
    "            print(f'  [!] DNA sequence is longer than the maximum specified ({len(dna_seq)} vs. {args.max_length} bp). Splitting the sequence...')\n",
    "\n",
    "            middle = len(dna_seq) / 2\n",
    "            ligation_sites = {}\n",
    "            for st in sticky_4bp:\n",
    "                matching_pos = np.array([match.start() for match in re.finditer(st, dna_seq)])\n",
    "\n",
    "                if len(matching_pos) == 0:\n",
    "                    # print(f'  No matche found for {st}')\n",
    "                    pass\n",
    "                else:\n",
    "                    closest_to_middle = matching_pos[np.argmin(np.abs(matching_pos - middle))]\n",
    "                    ligation_sites[closest_to_middle] = st\n",
    "                    # print(f'  Closest ligation site to the middle for {st} found at {closest_to_middle}')\n",
    "\n",
    "            if len(ligation_sites) == 0:\n",
    "                print('  [!] No cut site found for this sequence. This eBlock will be too long for synthesis. Ignoring it and moving on...')\n",
    "                print(f'  ################################################################################################################')\n",
    "                dna_fragments = {'':None}\n",
    "\n",
    "            else:\n",
    "                possible_sites = np.array(list(ligation_sites.keys()))\n",
    "                best = possible_sites[np.argmin(np.abs(possible_sites - middle))]\n",
    "                print(f'  * Closest ligation site to the middle of the sequence is {ligation_sites[best]} at {best}')\n",
    "\n",
    "                # Split DNA fragment and add new GG adapters\n",
    "                Nterm_dna, Cterm_dna = dna_seq[:best], dna_seq[best+4:]\n",
    "                dna_frag_x = Nterm_dna + ligation_sites[best] + gg_adapters[enzyme]['3prime']\n",
    "                dna_frag_y = gg_adapters[enzyme]['5prime'] + ligation_sites[best] + Cterm_dna\n",
    "                dna_fragments = {'x':dna_frag_x,'y':dna_frag_y}\n",
    "\n",
    "    return dna_fragments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-555c3e038aad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0maa_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morder_pdbs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mpdbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{args.order_pdbs}*.pdb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Extracting sequences from {len(pdbs)} PDBs...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# GET AA SEQUENCES\n",
    "# ============================================\n",
    "aa_sequences = {}\n",
    "\n",
    "if args.order_pdbs is not None:\n",
    "    pdbs = sorted(glob.glob(f'{args.order_pdbs}*.pdb'))\n",
    "    print(f'Extracting sequences from {len(pdbs)} PDBs...')\n",
    "\n",
    "    for pdb in pdbs:\n",
    "        pdb_name = pdb.split('/')[-1].replace('.pdb', '')\n",
    "        parser = PDB.PDBParser(PERMISSIVE=1, QUIET=True)\n",
    "        structure = parser.get_structure('design', pdb)\n",
    "        chain2seq = {}\n",
    "\n",
    "        for model in structure:\n",
    "             for chain in model:\n",
    "                chain2seq[chain.id] = ''\n",
    "\n",
    "                for residue in chain:\n",
    "                    chain2seq[chain.id] += SeqUtils.IUPACData.protein_letters_3to1[residue.resname.capitalize()]\n",
    "\n",
    "        seq2chain = {v:k for k, v in chain2seq.items()} # removes duplicates for homo-oligomers\n",
    "\n",
    "        if len(seq2chain) > 1:\n",
    "            is_heterooligomer = True\n",
    "\n",
    "        else:\n",
    "            is_heterooligomer = False\n",
    "\n",
    "        for seq, chain in seq2chain.items():\n",
    "            if is_heterooligomer:\n",
    "                aa_sequences[pdb_name + '_' + chain + '_isheterooligomer'] = seq  # for renaming later on.\n",
    "\n",
    "            else:\n",
    "                aa_sequences[pdb_name] = seq\n",
    "\n",
    "if args.order_fasta is not None:\n",
    "    print('Extracting sequences from FASTA file...')\n",
    "    fasta_sequences = list(SeqIO.parse(args.order_fasta, 'fasta'))\n",
    "    fasta_names = [fasta.id for fasta in fasta_sequences]\n",
    "\n",
    "    for fasta in fasta_sequences:\n",
    "\n",
    "        if fasta.id.split('_')[-1] in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ': # check if name has '_X' to indicate different chains.\n",
    "            base_name = '_'.join(fasta.id.split('_')[:-1])\n",
    "            num_bn = np.sum([True if base_name in fn else False for fn in fasta_names])\n",
    "\n",
    "            if num_bn > 1: # check if base name appears more than once.\n",
    "                aa_sequences[fasta.id + '_isheterooligomer'] = str(fasta.seq)\n",
    "\n",
    "            else:\n",
    "                aa_sequences[fasta.id] = str(fasta.seq)\n",
    "\n",
    "        else:\n",
    "            aa_sequences[fasta.id] = str(fasta.seq)\n",
    "\n",
    "print(f'Number of AA sequences extracted: {len(aa_sequences)}\\n')\n",
    "\n",
    "if len(aa_sequences) == 0:\n",
    "    print('No AA sequences found. System exiting...')\n",
    "    sys.exit()\n",
    "\n",
    "# Option for printing hetero-oligomeric sequences -- useful for debugging.\n",
    "if args.print_hto:\n",
    "    print('HETEROOLIGOMERS:')\n",
    "    heterooligos = np.unique(['_'.join(n.split('_')[:-2]) for n in aa_sequences.keys() if '_isheterooligomer' in n])\n",
    "    for hto in heterooligos:\n",
    "        for k, v in aa_sequences.items():\n",
    "            if hto in k:\n",
    "                print(f'>{k}\\n{v}')\n",
    "        print('-----')\n",
    "    print('\\n')\n",
    "\n",
    "# Check for duplicates\n",
    "all_aa_seq = len(aa_sequences)\n",
    "unique_aa_seq = len(np.unique(list(aa_sequences.values())))\n",
    "if all_aa_seq != unique_aa_seq:\n",
    "    print(f'[!] Found duplicated sequences ({unique_aa_seq} unique sequences vs. {all_aa_seq} total sequences):')\n",
    "\n",
    "    visited = set()\n",
    "    dup = [x for x in aa_sequences.values() if x in visited or (visited.add(x) or False)]\n",
    "    duplicates = {d:[] for d in dup}\n",
    "    for k, v in aa_sequences.items():\n",
    "        if v in dup:\n",
    "            duplicates[v].append(k)\n",
    "\n",
    "    for seq, ids in duplicates.items():\n",
    "        for id in ids:\n",
    "            print(f'>{id}')\n",
    "        print(seq + '\\n-----')\n",
    "\n",
    "    print('ERROR: Duplicates. Verify your sequences and retry. System exiting...')\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d085dd8ca044>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m'readin_order'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;34m'design_aa_seq'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34mf'{gg_v}_cloned_plasmid_seq'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgg_v\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgg_vector\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34mf'ORF_from_{gg_v}'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgg_v\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgg_vector\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34mf'exp_aa_seq_from_{gg_v}'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgg_v\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgg_vector\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# GENERATE eBLOCKS\n",
    "# ============================================\n",
    "eblocks = {\n",
    "    'design_name':[],\n",
    "    'Sequence':[],\n",
    "    'length_eblock':[],\n",
    "    'readin_order':[],\n",
    "    'design_aa_seq':[],\n",
    "    **{f'{gg_v}_cloned_plasmid_seq':[] for gg_v in args.gg_vector},\n",
    "    **{f'ORF_from_{gg_v}':[] for gg_v in args.gg_vector},\n",
    "    **{f'exp_aa_seq_from_{gg_v}':[] for gg_v in args.gg_vector},\n",
    "    'idt_score':[]\n",
    "    }\n",
    "\n",
    "skipped = {}\n",
    "for i, (design_name, aa_seq) in enumerate(aa_sequences.items()):\n",
    "\n",
    "    print(f'>>> [{i+1}/{len(aa_sequences)}] {design_name.replace(\"_isheterooligomer\", \"\")}: {aa_seq}')\n",
    "\n",
    "    output = True\n",
    "    n_dom_steps = 0\n",
    "    kmers_weight_settings = np.linspace(args.starting_kmers_weight, 100, args.n_domesticator_steps)\n",
    "    idt_score = 10.0\n",
    "    while (idt_score >= args.idt_score) and (n_dom_steps < args.n_domesticator_steps) and (output == True):\n",
    "\n",
    "        # Reverse translate\n",
    "        print(f'  Reverse translating (optimising for {args.species} with kmers_weight@{kmers_weight_settings[n_dom_steps]})...')\n",
    "\n",
    "        dna_seq = domesticator.reverse_translate(\n",
    "                                    aa_seq,\n",
    "                                    kmers_weight=kmers_weight_settings[n_dom_steps],\n",
    "                                    cai_weight=1.0,\n",
    "                                    hairpins_weight=1.0,\n",
    "                                    max_tries=args.max_attempts,\n",
    "                                    species=args.species,\n",
    "                                    avoid=avoid_seq[enzyme]\n",
    "                                    )\n",
    "        n_dom_steps += 1\n",
    "\n",
    "        # Double-check for cut sites.\n",
    "        for seq in avoid_seq[enzyme]:\n",
    "            if seq in dna_seq.upper():\n",
    "                output = False\n",
    "                skipped[design_name + f' [ERROR: {enzyme} site(s) detected]'] = aa_seq\n",
    "                print(f'  {dna_seq}')\n",
    "                print(f'  [!] {enzyme} site(s) detected. Problem with reverse translation!')\n",
    "                print(f'  ############################################################')\n",
    "\n",
    "            else:\n",
    "                output = True\n",
    "\n",
    "        # Add GG adapters and vector-specific sticky ends to DNA sequence.\n",
    "        dna_seq = gg_adapters[enzyme]['5prime'] + sticky5prime + dna_seq + sticky3prime + gg_adapters[enzyme]['3prime']\n",
    "\n",
    "        # Check that the sequence fits an eBlock. If not, pad or split the sequence to fall within the 300-1500 bp limits.\n",
    "        dna_fragments = adjust_for_eblock(dna_seq, args.max_length, gg_adapters[enzyme], avoid_seq[enzyme])\n",
    "\n",
    "        if None in dna_fragments.values():\n",
    "            output = False\n",
    "            skipped[design_name + f' [ERROR: too large to be ordered as eBlocks]'] = aa_seq\n",
    "\n",
    "        else:\n",
    "\n",
    "            if args.skip_idt_query:\n",
    "                idt_scores = {}\n",
    "                for f, dna_frag in dna_fragments.items():\n",
    "                    idt_scores[f] = 0 # to escape while loop\n",
    "\n",
    "                idt_score = np.max(list(idt_scores.values()))\n",
    "                print(f'  Skipping IDT query.')\n",
    "\n",
    "            else:\n",
    "                # Check the synthesiability score(s) of the fragments.\n",
    "                idt_scores = {}\n",
    "                for f, dna_frag in dna_fragments.items():\n",
    "                    idt_scores[f] = idt.total_score(dna_frag, idt_user_info)\n",
    "\n",
    "                idt_score = np.max(list(idt_scores.values())) # for splitted genes, take the worst score of the two.\n",
    "\n",
    "                if idt_score >= args.idt_score:\n",
    "                    print(f'  [!] DNA sequence(s) failed the complexity test from IDT (Total Complexity Score = {idt_score:.1f}). Trying again...')\n",
    "\n",
    "                else:\n",
    "                    print(f'  DNA sequence(s) passed the complexity test from IDT (Total Complexity Score = {idt_score:.1f}).')\n",
    "\n",
    "\n",
    "        if n_dom_steps >= args.n_domesticator_steps:\n",
    "            output = False\n",
    "            skipped[design_name + f' [ERROR: IDT cannot make it]'] = aa_seq\n",
    "            print(f'  [!] Maximum number of Domesticator steps reached ({args.n_domesticator_steps}).')\n",
    "            print(f\"  [!] This design could not be reverse translated to IDT's specifications. Ignoring it and moving on...\")\n",
    "            print(f'  #####################################################################################################')\n",
    "\n",
    "    # Print/save results.\n",
    "    if output == False:\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        print(f'  Final eBlock(s):')\n",
    "        for f, dna_frag in dna_fragments.items():\n",
    "            print(f'  - eBlock {f.upper()} ({len(dna_frag)} bp): {dna_frag}')\n",
    "\n",
    "            \n",
    "        # Save results.\n",
    "        for f, dna_frag in dna_fragments.items():\n",
    "            eblocks['design_name'].append(design_name)\n",
    "            eblocks['Sequence'].append(dna_frag)\n",
    "            eblocks['length_eblock'].append(len(dna_frag))\n",
    "            eblocks['readin_order'].append(str(i) + f)\n",
    "            eblocks['design_aa_seq'].append(aa_seq)\n",
    "            eblocks['idt_score'].append(idt_scores[f])\n",
    "\n",
    "\n",
    "            for gg_v in args.gg_vector:\n",
    "                \n",
    "                # Enzyme-specific cut characteristics.\n",
    "                fw, rv, n_spacer, n_sticky = cuts[enzyme]\n",
    "\n",
    "                # GG cloning.\n",
    "                entry_vector = entry_vectors[gg_v]\n",
    "                vector_5prime, vector_3prime = entry_vector.find(fw) \\\n",
    "                                                + len(fw) \\\n",
    "                                                + n_spacer, entry_vector.find(rv) \\\n",
    "                                                - n_spacer\n",
    "\n",
    "                insert = dna_seq.lower()\n",
    "\n",
    "                # Find eBlock section with cut sites facing in the correct directions.\n",
    "                # (Necessary for cases where cut sites are accidentlly also present in the padding regions.)\n",
    "                fw_locations = np.array([x.span()[0] for x in re.finditer(fw, insert)])\n",
    "                rv_locations = np.array([x.span()[0] for x in re.finditer(rv, insert)])\n",
    "\n",
    "                fw_idx = []\n",
    "                rv_idx = []\n",
    "                delta_bp = []\n",
    "                for i, f in enumerate(fw_locations):\n",
    "                    for j, r in enumerate(rv_locations):\n",
    "                        delta_bp.append(r - f)\n",
    "                        fw_idx.append(i)\n",
    "                        rv_idx.append(j)\n",
    "\n",
    "                delta_bp = np.array(delta_bp)\n",
    "                correct_idx = np.argwhere(delta_bp==delta_bp[delta_bp>=3*len(aa_seq)].min())[0][0]\n",
    "\n",
    "                insert_5prime = fw_locations[fw_idx[correct_idx]] \\\n",
    "                                + len(fw) \\\n",
    "                                + n_spacer \\\n",
    "                                + n_sticky\n",
    "\n",
    "                insert_3prime = rv_locations[rv_idx[correct_idx]] \\\n",
    "                                - n_spacer \\\n",
    "                                - n_sticky\n",
    "\n",
    "                assembled_plasmid = entry_vector[:vector_3prime] \\\n",
    "                                        + insert[insert_5prime:insert_3prime] \\\n",
    "                                        + entry_vector[vector_5prime:]\n",
    "                \n",
    "                eblocks[f'{gg_v}_cloned_plasmid_seq'].append(assembled_plasmid.lower())\n",
    "                \n",
    "                \n",
    "                # Identify the ORF that contains the insert.\n",
    "                # Search for the shortest START-STOP span that contains the insert sequence.\n",
    "                plasmid_seq = assembled_plasmid.lower()\n",
    "                starts = np.array([s.start() for s in re.finditer('atg', plasmid_seq)])\n",
    "                ends = np.array(sorted([e.end() for e in re.finditer('tag', plasmid_seq)] \n",
    "                                       + [e.end() for e in re.finditer('taa', plasmid_seq)] \n",
    "                                       + [e.end() for e in re.finditer('tga', plasmid_seq)]))\n",
    "                current_stop = 0\n",
    "\n",
    "                for s in starts:\n",
    "                    inframe_stops = ends[np.logical_and(ends>s, (ends-s)%3==0)]\n",
    "\n",
    "                    if len(inframe_stops) > 0:\n",
    "                        if s > current_stop:\n",
    "                            current_stop = inframe_stops[0]\n",
    "                            coding_seq = plasmid_seq[s:current_stop]\n",
    "\n",
    "                            if insert[insert_5prime:insert_3prime] in coding_seq:\n",
    "                                ORF = coding_seq\n",
    "                                exp_product = str(Seq.Seq(ORF).translate())\n",
    "                \n",
    "                eblocks[f'ORF_from_{gg_v}'].append(ORF)\n",
    "                eblocks[f'exp_aa_seq_from_{gg_v}'].append(exp_product)\n",
    "        \n",
    "                print(f'  - Expression product (if cloned into {gg_v} with {enzyme}): {exp_product}')\n",
    "          \n",
    "    print('\\n')\n",
    "    \n",
    "\n",
    "df = pd.DataFrame.from_dict(eblocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a6ef67b8034a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Find fragments and heterooligomers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mdesign_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'design_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'order_name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morder_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_frag'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'readin_order'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'y'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# PLATE FORMATTING\n",
    "# ============================================\n",
    "# For 'no_layout' option (just fill plate from A1->H12)\n",
    "w96 = [str(p) + '_' + r + str(c) for p in range(1,10) for r in 'ABCDEFGH' for c in range(1,13)]\n",
    "\n",
    "# For 384w formatting (no layout)\n",
    "w384 = [str(p) + '_' + r + str(c) for p in range(1,10) for r in 'ABCDEFGHIJKLMNOP' for c in range(1,25)]\n",
    "w384_df = pd.DataFrame([r + str(c) for r in 'ABCDEFGHIJKLMNOP' for c in range(1,25)], columns=['Well Position'])\n",
    "\n",
    "# For single fragments plate layouts -- staggered arrangement for gel loading.\n",
    "w96_zigzag = []\n",
    "for p in range(1, 10):\n",
    "    for pair in ['AB', 'CD', 'EF', 'GH']:\n",
    "        for c in range(1,13):\n",
    "            for r in pair:\n",
    "                w96_zigzag.append(str(p) + '_' + r + str(c))\n",
    "\n",
    "\n",
    "# For ECHO transfer file generation\n",
    "w96_zigzag_single = []\n",
    "for pair in ['AB', 'CD', 'EF', 'GH']:\n",
    "    for c in range(1,13):\n",
    "        for r in pair:\n",
    "            w96_zigzag_single.append(r + str(c))\n",
    "\n",
    "\n",
    "# For dual fragments and hetero-dimer layouts -- two horizontal blcoks of 48.\n",
    "w96_2blocks = []\n",
    "for p in range(1, 10):\n",
    "    for pair in ['AE', 'BF', 'CG', 'DH']:\n",
    "        for c in range(1,13):\n",
    "            for r in pair:\n",
    "                w96_2blocks.append(str(p) + '_' + r + str(c))\n",
    "\n",
    "# For triple fragments and hetero-trimer layouts -- three vertical blocks of 32.\n",
    "w96_3blocks = []\n",
    "for p in range(1, 10):\n",
    "    for group in ['1 5 9', '2 6 10', '3 7 11', '4 8 12']:\n",
    "        for r in 'ABCDEFGH':\n",
    "            for c in group.split():\n",
    "                w96_3blocks.append(str(p) + '_' + r + c)\n",
    "\n",
    "# For quadruple fragments and hetero-tetramer layouts -- four horizontal blocks of 24.\n",
    "w96_4blocks = []\n",
    "for p in range(1, 10):\n",
    "    for pair in ['AE', 'BF', 'CG', 'DH']:\n",
    "        for c in range(1,13):\n",
    "            for r in pair:\n",
    "                w96_4blocks.append(str(p) + '_' + r + str(c))\n",
    "\n",
    "# Find fragments and heterooligomers.\n",
    "design_names = df['design_name'].values\n",
    "df['order_name'] = len(df) * [args.order_name]\n",
    "df['is_frag'] = df['readin_order'].apply(lambda x: True if ('x' in x) or ('y' in x) else False )\n",
    "df['is_hto'] = df['design_name'].apply(lambda x: True if '_isheterooligomer' in x else False)\n",
    "\n",
    "# Split eBlocks into categories for individual reformatting.\n",
    "single_eblocks = df[(df['is_frag']==False) & (df['is_hto']==False)]\n",
    "frag_eblocks = df[(df['is_frag']==True) & (df['is_hto']==False)]\n",
    "hto_eblocks = df[(df['is_frag']==False) & (df['is_hto']==True)]\n",
    "htd_frag_eblocks = df[(df['is_frag']==True) & (df['is_hto']==True)]\n",
    "\n",
    "n_designs = int(args.design_id)\n",
    "n_plates = int(0)\n",
    "\n",
    " # Reformat single eBlocks.\n",
    "if len(single_eblocks) > 0:\n",
    "    design_ids = [args.design_prefix + str(i).zfill(4) for i in range(n_designs, n_designs + len(single_eblocks))]\n",
    "    single_eblocks['design_id'] = design_ids\n",
    "    single_eblocks['position'] = w96_zigzag[:len(single_eblocks)]\n",
    "    single_eblocks['plate_id'] = single_eblocks['position'].apply(lambda x: int(x.split('_')[0]) + n_plates)\n",
    "    single_eblocks['Well Position'] = single_eblocks['position'].apply(lambda x: x.split('_')[1])\n",
    "\n",
    "    n_designs += int(len(single_eblocks))\n",
    "    n_plates = int(single_eblocks['plate_id'].max())\n",
    "\n",
    "# Re-format fragment eBlocks.\n",
    "if len(frag_eblocks) > 0:\n",
    "    unique_idx = np.unique([v[:-1] for v in frag_eblocks['readin_order'].values])\n",
    "    oldidx2newidx = {old:n_designs + i for i, old in enumerate(unique_idx)}\n",
    "    frag_eblocks['design_id'] = frag_eblocks['readin_order'].apply(lambda x: args.design_prefix + str(oldidx2newidx[x[:-1]]).zfill(4) + x[-1])\n",
    "    frag_eblocks['position'] = w96_2blocks[:len(frag_eblocks)]\n",
    "    frag_eblocks['plate_id'] = frag_eblocks['position'].apply(lambda x: int(x.split('_')[0]) + n_plates)\n",
    "    frag_eblocks['Well Position'] = frag_eblocks['position'].apply(lambda x: x.split('_')[1])\n",
    "\n",
    "    n_designs += int(len(frag_eblocks) / 2)\n",
    "    n_plates = int(frag_eblocks['plate_id'].max())\n",
    "\n",
    "# Reformat heterooligomer eBlocks.\n",
    "if len(hto_eblocks) > 0:\n",
    "\n",
    "    # Check oligomeric stoichiometries.\n",
    "    chains = [dn.split('_')[-2] for dn in hto_eblocks['design_name'].values] # get all chains\n",
    "    n_each_chain = [chains.count(ch) for ch in np.unique(chains)]\n",
    "    if len(np.unique(n_each_chain)) > 1:\n",
    "        print('No plate formatting currently implemented for mixed stoichiometry heterooligomers. Chains have different IDs and the layout is the same as for monomers.')\n",
    "        design_ids = [args.design_prefix + str(i).zfill(4) for i in range(n_designs, n_designs + len(hto_eblocks))]\n",
    "        hto_eblocks['design_id'] = design_ids\n",
    "        hto_eblocks['position'] = w96_zigzag[:len(hto_eblocks)]\n",
    "        hto_eblocks['plate_id'] = hto_eblocks['position'].apply(lambda x: int(x.split('_')[0]) + n_plates)\n",
    "        hto_eblocks['Well Position'] = hto_eblocks['position'].apply(lambda x: x.split('_')[1])\n",
    "\n",
    "    else:\n",
    "        n_unique_ch = len(np.unique(chains))\n",
    "        ch_idx = []\n",
    "        for ch in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'[:n_unique_ch]:\n",
    "            ch_idx += [args.design_prefix + str(i).zfill(4) + f'_{ch}' for i in range(n_designs, n_designs+int(len(hto_eblocks)/n_unique_ch))]\n",
    "\n",
    "        hto_eblocks['design_id'] = np.hstack(np.reshape(ch_idx, (n_unique_ch, -1)).T)\n",
    "\n",
    "        if n_unique_ch > 4:\n",
    "            print('No plate formatting for heterooligomers beyond tetramers currently implemented. The layout is the same as for monomers.')\n",
    "            hto_eblocks['position'] = w96_zigzag[:len(hto_eblocks)]\n",
    "\n",
    "        else:\n",
    "            if n_unique_ch == 2:\n",
    "                hto_eblocks['position'] = w96_2blocks[:len(hto_eblocks)]\n",
    "            elif n_unique_ch == 3:\n",
    "                hto_eblocks['position'] = w96_3blocks[:len(hto_eblocks)]\n",
    "            elif n_unique_ch == 4:\n",
    "                hto_eblocks['position'] = w96_4blocks[:len(hto_eblocks)]\n",
    "\n",
    "    hto_eblocks['plate_id'] = hto_eblocks['position'].apply(lambda x: int(x.split('_')[0]) + n_plates)\n",
    "    hto_eblocks['Well Position'] = hto_eblocks['position'].apply(lambda x: x.split('_')[1])\n",
    "\n",
    "\n",
    "    n_designs += int(len(hto_eblocks) / 2)\n",
    "    n_plates = int(hto_eblocks['plate_id'].max())\n",
    "\n",
    "# Reformat heterooligomer eBlocks that have been fragmented.\n",
    "if len(htd_frag_eblocks) > 0:\n",
    "    print('No plate formatting currently implemented for fragmented heterooligomers. Chains have different IDs and the layout is the same as for fragmented monomers.')\n",
    "    unique_idx = np.unique([v[:-1] for v in htd_frag_eblocks['readin_order'].values])\n",
    "    oldidx2newidx = {old:n_designs + i for i, old in enumerate(unique_idx)}\n",
    "    htd_frag_eblocks['design_id'] = htd_frag_eblocks['readin_order'].apply(lambda x: args.design_prefix + str(oldidx2newidx[x[:-1]]).zfill(4) + x[-1])\n",
    "    htd_frag_eblocks['position'] = w96_2blocks[:len(htd_frag_eblocks)]\n",
    "    htd_frag_eblocks['plate_id'] = htd_frag_eblocks['position'].apply(lambda x: int(x.split('_')[0]) + n_plates)\n",
    "    htd_frag_eblocks['Well Position'] = htd_frag_eblocks['position'].apply(lambda x: x.split('_')[1])\n",
    "\n",
    "    n_designs += int(len(htd_frag_eblocks) / 2)\n",
    "    n_plates = int(htd_frag_eblocks['plate_id'].max())\n",
    "\n",
    "\n",
    "reformated_df = pd.concat([single_eblocks, frag_eblocks, hto_eblocks, htd_frag_eblocks])\n",
    "\n",
    "if args.no_layout:\n",
    "    if args.echo:\n",
    "\n",
    "        def gen_design_id(r):\n",
    "            if ('x' in r.readin_order) or ('y' in r.readin_order):\n",
    "                return args.design_prefix + str(int(args.design_id) + int(r.readin_order[:-1])).zfill(4) + r.readin_order[-1]\n",
    "\n",
    "            else:\n",
    "                return args.design_prefix + str(int(args.design_id) + int(r.readin_order)).zfill(4)\n",
    "\n",
    "        df['design_id'] = df.apply(gen_design_id, axis=1)\n",
    "        df['position'] = w384[:len(df)]\n",
    "        df['plate_id'] = df['position'].apply(lambda x: int(x.split('_')[0]))\n",
    "        df['Well Position'] = df['position'].apply(lambda x: x.split('_')[1])\n",
    "\n",
    "        reformated_df = df.copy()\n",
    "\n",
    "        # Generate ECHO transfer file -- currently only to 1frag/1vector type of cloning.\n",
    "        df['Source Plate Name'] = ''\n",
    "        df['Source Well'] = df['Well Position']\n",
    "        for p in df['plate_id'].unique():\n",
    "            df.loc[df['plate_id']==p, 'Source Plate Name'] = f'{date}_{args.order_name}.{int(p)}_{args.species}_{enzyme}'\n",
    "\n",
    "        # Assumes: 348w (eBlocks) --> 96w (cultures)\n",
    "        split_idx = [df.index.values[x:x+96] for x in range(0, len(df), 96)]\n",
    "        df['Destination Plate Name'] = ''\n",
    "        df['Destination Well'] = ''\n",
    "        for i, si in enumerate(split_idx):\n",
    "            df.loc[si, 'Destination Plate Name'] = f'PCR96 - {i+1}'\n",
    "            df.loc[si, 'Destination Well'] = w96_zigzag_single[:len(si)]\n",
    "\n",
    "        # GG mastermix: hard-coded defintions.\n",
    "        tot_rxn_vol = 1 # uL\n",
    "        eblock_conc = 4 # ng/uL -- as delivered by IDT\n",
    "        vector_conc = 100 # ng/uL -- assumes vector stocks have been normalised\n",
    "        mol_vector = 4 # fmol, target amount\n",
    "        insert_to_vector = 2 # target insert:vector molar ratio\n",
    "\n",
    "        eblock_bp = df['length_eblock'].median()\n",
    "        vector_bp = len([str(r.seq) for r in SeqIO.parse(glob.glob(f'/software/lab/johnbercow/entry_vectors/{args.gg_vector[0]}*.fa')[0], 'fasta')][0]) # only considers first vector if multiple were specified\n",
    "        vector_vol = mol_vector / (((vector_conc * 0.000000001) / ((vector_bp * 617.96) + 36.04)) * 1000000000000000)\n",
    "        eblock_vol = 0.025 * np.ceil(((mol_vector * insert_to_vector) / (((eblock_conc * 0.000000001) / ((eblock_bp * 617.96) + 36.04)) * 1000000000000000)) / 0.025)\n",
    "        mm_vol = tot_rxn_vol - eblock_vol\n",
    "\n",
    "        echo_df = df[['Source Plate Name', 'Source Well', 'Destination Plate Name', 'Destination Well']].copy()\n",
    "        echo_df['Transfer Volume'] = eblock_vol * 1e3 # transfer volumes are specified in nL\n",
    "        echo_df2 = echo_df.copy() # for specifying GGMM destinations\n",
    "        echo_df2['Source Well'] = 'X' # replace manually later\n",
    "        echo_df2.loc[:, 'Transfer Volume'] = mm_vol * 1e3\n",
    "\n",
    "        echo_df = pd.concat([echo_df, echo_df2])\n",
    "        echo_df.to_csv(f'{filename}_ECHO.csv', index=False)\n",
    "\n",
    "\n",
    "    else:\n",
    "        design_ids = [args.design_prefix + str(i).zfill(4) for i in range(int(args.design_id), int(args.design_id) + len(df))]\n",
    "        df['design_id'] = design_ids\n",
    "        df['position'] = w96[:len(df)]\n",
    "        df['plate_id'] = df['position'].apply(lambda x: int(x.split('_')[0]))\n",
    "        df['Well Position'] = df['position'].apply(lambda x: x.split('_')[1])\n",
    "        reformated_df = df.copy()\n",
    "\n",
    "reformated_df = reformated_df.astype({'plate_id':'int32'})\n",
    "\n",
    "# Change some characters in design names otherwise IDT complains.\n",
    "change_char = {\n",
    "'_isheterooligomer':'',\n",
    "'+':'plus'\n",
    "}\n",
    "def name(row):\n",
    "\n",
    "    clean_name = row['design_name']\n",
    "    for k, v in change_char.items():\n",
    "        clean_name = clean_name.replace(k, v)\n",
    "\n",
    "    return row['design_id'] + '__' + row['Well Position'] + '__' + row['order_name'] + '.' + str(int(row['plate_id'])) + '__' + clean_name\n",
    "\n",
    "reformated_df['Name'] = reformated_df.apply(name, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1_A1',\n",
       " '1_E1',\n",
       " '1_A2',\n",
       " '1_E2',\n",
       " '1_A3',\n",
       " '1_E3',\n",
       " '1_A4',\n",
       " '1_E4',\n",
       " '1_A5',\n",
       " '1_E5',\n",
       " '1_A6',\n",
       " '1_E6',\n",
       " '1_A7',\n",
       " '1_E7',\n",
       " '1_A8',\n",
       " '1_E8',\n",
       " '1_A9',\n",
       " '1_E9',\n",
       " '1_A10',\n",
       " '1_E10',\n",
       " '1_A11',\n",
       " '1_E11',\n",
       " '1_A12',\n",
       " '1_E12',\n",
       " '1_B1',\n",
       " '1_F1',\n",
       " '1_B2',\n",
       " '1_F2',\n",
       " '1_B3',\n",
       " '1_F3',\n",
       " '1_B4',\n",
       " '1_F4',\n",
       " '1_B5',\n",
       " '1_F5',\n",
       " '1_B6',\n",
       " '1_F6',\n",
       " '1_B7',\n",
       " '1_F7',\n",
       " '1_B8',\n",
       " '1_F8',\n",
       " '1_B9',\n",
       " '1_F9',\n",
       " '1_B10',\n",
       " '1_F10',\n",
       " '1_B11',\n",
       " '1_F11',\n",
       " '1_B12',\n",
       " '1_F12',\n",
       " '1_C1',\n",
       " '1_G1',\n",
       " '1_C2',\n",
       " '1_G2',\n",
       " '1_C3',\n",
       " '1_G3',\n",
       " '1_C4',\n",
       " '1_G4',\n",
       " '1_C5',\n",
       " '1_G5',\n",
       " '1_C6',\n",
       " '1_G6',\n",
       " '1_C7',\n",
       " '1_G7',\n",
       " '1_C8',\n",
       " '1_G8',\n",
       " '1_C9',\n",
       " '1_G9',\n",
       " '1_C10',\n",
       " '1_G10',\n",
       " '1_C11',\n",
       " '1_G11',\n",
       " '1_C12',\n",
       " '1_G12',\n",
       " '1_D1',\n",
       " '1_H1',\n",
       " '1_D2',\n",
       " '1_H2',\n",
       " '1_D3',\n",
       " '1_H3',\n",
       " '1_D4',\n",
       " '1_H4',\n",
       " '1_D5',\n",
       " '1_H5',\n",
       " '1_D6',\n",
       " '1_H6',\n",
       " '1_D7',\n",
       " '1_H7',\n",
       " '1_D8',\n",
       " '1_H8',\n",
       " '1_D9',\n",
       " '1_H9',\n",
       " '1_D10',\n",
       " '1_H10',\n",
       " '1_D11',\n",
       " '1_H11',\n",
       " '1_D12',\n",
       " '1_H12',\n",
       " '2_A1',\n",
       " '2_E1',\n",
       " '2_A2',\n",
       " '2_E2',\n",
       " '2_A3',\n",
       " '2_E3',\n",
       " '2_A4',\n",
       " '2_E4',\n",
       " '2_A5',\n",
       " '2_E5',\n",
       " '2_A6',\n",
       " '2_E6',\n",
       " '2_A7',\n",
       " '2_E7',\n",
       " '2_A8',\n",
       " '2_E8',\n",
       " '2_A9',\n",
       " '2_E9',\n",
       " '2_A10',\n",
       " '2_E10',\n",
       " '2_A11',\n",
       " '2_E11',\n",
       " '2_A12',\n",
       " '2_E12',\n",
       " '2_B1',\n",
       " '2_F1',\n",
       " '2_B2',\n",
       " '2_F2',\n",
       " '2_B3',\n",
       " '2_F3',\n",
       " '2_B4',\n",
       " '2_F4',\n",
       " '2_B5',\n",
       " '2_F5',\n",
       " '2_B6',\n",
       " '2_F6',\n",
       " '2_B7',\n",
       " '2_F7',\n",
       " '2_B8',\n",
       " '2_F8',\n",
       " '2_B9',\n",
       " '2_F9',\n",
       " '2_B10',\n",
       " '2_F10',\n",
       " '2_B11',\n",
       " '2_F11',\n",
       " '2_B12',\n",
       " '2_F12',\n",
       " '2_C1',\n",
       " '2_G1',\n",
       " '2_C2',\n",
       " '2_G2',\n",
       " '2_C3',\n",
       " '2_G3',\n",
       " '2_C4',\n",
       " '2_G4',\n",
       " '2_C5',\n",
       " '2_G5',\n",
       " '2_C6',\n",
       " '2_G6',\n",
       " '2_C7',\n",
       " '2_G7',\n",
       " '2_C8',\n",
       " '2_G8',\n",
       " '2_C9',\n",
       " '2_G9',\n",
       " '2_C10',\n",
       " '2_G10',\n",
       " '2_C11',\n",
       " '2_G11',\n",
       " '2_C12',\n",
       " '2_G12',\n",
       " '2_D1',\n",
       " '2_H1',\n",
       " '2_D2',\n",
       " '2_H2',\n",
       " '2_D3',\n",
       " '2_H3',\n",
       " '2_D4',\n",
       " '2_H4',\n",
       " '2_D5',\n",
       " '2_H5',\n",
       " '2_D6',\n",
       " '2_H6',\n",
       " '2_D7',\n",
       " '2_H7',\n",
       " '2_D8',\n",
       " '2_H8',\n",
       " '2_D9',\n",
       " '2_H9',\n",
       " '2_D10',\n",
       " '2_H10',\n",
       " '2_D11',\n",
       " '2_H11',\n",
       " '2_D12',\n",
       " '2_H12',\n",
       " '3_A1',\n",
       " '3_E1',\n",
       " '3_A2',\n",
       " '3_E2',\n",
       " '3_A3',\n",
       " '3_E3',\n",
       " '3_A4',\n",
       " '3_E4',\n",
       " '3_A5',\n",
       " '3_E5',\n",
       " '3_A6',\n",
       " '3_E6',\n",
       " '3_A7',\n",
       " '3_E7',\n",
       " '3_A8',\n",
       " '3_E8',\n",
       " '3_A9',\n",
       " '3_E9',\n",
       " '3_A10',\n",
       " '3_E10',\n",
       " '3_A11',\n",
       " '3_E11',\n",
       " '3_A12',\n",
       " '3_E12',\n",
       " '3_B1',\n",
       " '3_F1',\n",
       " '3_B2',\n",
       " '3_F2',\n",
       " '3_B3',\n",
       " '3_F3',\n",
       " '3_B4',\n",
       " '3_F4',\n",
       " '3_B5',\n",
       " '3_F5',\n",
       " '3_B6',\n",
       " '3_F6',\n",
       " '3_B7',\n",
       " '3_F7',\n",
       " '3_B8',\n",
       " '3_F8',\n",
       " '3_B9',\n",
       " '3_F9',\n",
       " '3_B10',\n",
       " '3_F10',\n",
       " '3_B11',\n",
       " '3_F11',\n",
       " '3_B12',\n",
       " '3_F12',\n",
       " '3_C1',\n",
       " '3_G1',\n",
       " '3_C2',\n",
       " '3_G2',\n",
       " '3_C3',\n",
       " '3_G3',\n",
       " '3_C4',\n",
       " '3_G4',\n",
       " '3_C5',\n",
       " '3_G5',\n",
       " '3_C6',\n",
       " '3_G6',\n",
       " '3_C7',\n",
       " '3_G7',\n",
       " '3_C8',\n",
       " '3_G8',\n",
       " '3_C9',\n",
       " '3_G9',\n",
       " '3_C10',\n",
       " '3_G10',\n",
       " '3_C11',\n",
       " '3_G11',\n",
       " '3_C12',\n",
       " '3_G12',\n",
       " '3_D1',\n",
       " '3_H1',\n",
       " '3_D2',\n",
       " '3_H2',\n",
       " '3_D3',\n",
       " '3_H3',\n",
       " '3_D4',\n",
       " '3_H4',\n",
       " '3_D5',\n",
       " '3_H5',\n",
       " '3_D6',\n",
       " '3_H6',\n",
       " '3_D7',\n",
       " '3_H7',\n",
       " '3_D8',\n",
       " '3_H8',\n",
       " '3_D9',\n",
       " '3_H9',\n",
       " '3_D10',\n",
       " '3_H10',\n",
       " '3_D11',\n",
       " '3_H11',\n",
       " '3_D12',\n",
       " '3_H12',\n",
       " '4_A1',\n",
       " '4_E1',\n",
       " '4_A2',\n",
       " '4_E2',\n",
       " '4_A3',\n",
       " '4_E3',\n",
       " '4_A4',\n",
       " '4_E4',\n",
       " '4_A5',\n",
       " '4_E5',\n",
       " '4_A6',\n",
       " '4_E6',\n",
       " '4_A7',\n",
       " '4_E7',\n",
       " '4_A8',\n",
       " '4_E8',\n",
       " '4_A9',\n",
       " '4_E9',\n",
       " '4_A10',\n",
       " '4_E10',\n",
       " '4_A11',\n",
       " '4_E11',\n",
       " '4_A12',\n",
       " '4_E12',\n",
       " '4_B1',\n",
       " '4_F1',\n",
       " '4_B2',\n",
       " '4_F2',\n",
       " '4_B3',\n",
       " '4_F3',\n",
       " '4_B4',\n",
       " '4_F4',\n",
       " '4_B5',\n",
       " '4_F5',\n",
       " '4_B6',\n",
       " '4_F6',\n",
       " '4_B7',\n",
       " '4_F7',\n",
       " '4_B8',\n",
       " '4_F8',\n",
       " '4_B9',\n",
       " '4_F9',\n",
       " '4_B10',\n",
       " '4_F10',\n",
       " '4_B11',\n",
       " '4_F11',\n",
       " '4_B12',\n",
       " '4_F12',\n",
       " '4_C1',\n",
       " '4_G1',\n",
       " '4_C2',\n",
       " '4_G2',\n",
       " '4_C3',\n",
       " '4_G3',\n",
       " '4_C4',\n",
       " '4_G4',\n",
       " '4_C5',\n",
       " '4_G5',\n",
       " '4_C6',\n",
       " '4_G6',\n",
       " '4_C7',\n",
       " '4_G7',\n",
       " '4_C8',\n",
       " '4_G8',\n",
       " '4_C9',\n",
       " '4_G9',\n",
       " '4_C10',\n",
       " '4_G10',\n",
       " '4_C11',\n",
       " '4_G11',\n",
       " '4_C12',\n",
       " '4_G12',\n",
       " '4_D1',\n",
       " '4_H1',\n",
       " '4_D2',\n",
       " '4_H2',\n",
       " '4_D3',\n",
       " '4_H3',\n",
       " '4_D4',\n",
       " '4_H4',\n",
       " '4_D5',\n",
       " '4_H5',\n",
       " '4_D6',\n",
       " '4_H6',\n",
       " '4_D7',\n",
       " '4_H7',\n",
       " '4_D8',\n",
       " '4_H8',\n",
       " '4_D9',\n",
       " '4_H9',\n",
       " '4_D10',\n",
       " '4_H10',\n",
       " '4_D11',\n",
       " '4_H11',\n",
       " '4_D12',\n",
       " '4_H12',\n",
       " '5_A1',\n",
       " '5_E1',\n",
       " '5_A2',\n",
       " '5_E2',\n",
       " '5_A3',\n",
       " '5_E3',\n",
       " '5_A4',\n",
       " '5_E4',\n",
       " '5_A5',\n",
       " '5_E5',\n",
       " '5_A6',\n",
       " '5_E6',\n",
       " '5_A7',\n",
       " '5_E7',\n",
       " '5_A8',\n",
       " '5_E8',\n",
       " '5_A9',\n",
       " '5_E9',\n",
       " '5_A10',\n",
       " '5_E10',\n",
       " '5_A11',\n",
       " '5_E11',\n",
       " '5_A12',\n",
       " '5_E12',\n",
       " '5_B1',\n",
       " '5_F1',\n",
       " '5_B2',\n",
       " '5_F2',\n",
       " '5_B3',\n",
       " '5_F3',\n",
       " '5_B4',\n",
       " '5_F4',\n",
       " '5_B5',\n",
       " '5_F5',\n",
       " '5_B6',\n",
       " '5_F6',\n",
       " '5_B7',\n",
       " '5_F7',\n",
       " '5_B8',\n",
       " '5_F8',\n",
       " '5_B9',\n",
       " '5_F9',\n",
       " '5_B10',\n",
       " '5_F10',\n",
       " '5_B11',\n",
       " '5_F11',\n",
       " '5_B12',\n",
       " '5_F12',\n",
       " '5_C1',\n",
       " '5_G1',\n",
       " '5_C2',\n",
       " '5_G2',\n",
       " '5_C3',\n",
       " '5_G3',\n",
       " '5_C4',\n",
       " '5_G4',\n",
       " '5_C5',\n",
       " '5_G5',\n",
       " '5_C6',\n",
       " '5_G6',\n",
       " '5_C7',\n",
       " '5_G7',\n",
       " '5_C8',\n",
       " '5_G8',\n",
       " '5_C9',\n",
       " '5_G9',\n",
       " '5_C10',\n",
       " '5_G10',\n",
       " '5_C11',\n",
       " '5_G11',\n",
       " '5_C12',\n",
       " '5_G12',\n",
       " '5_D1',\n",
       " '5_H1',\n",
       " '5_D2',\n",
       " '5_H2',\n",
       " '5_D3',\n",
       " '5_H3',\n",
       " '5_D4',\n",
       " '5_H4',\n",
       " '5_D5',\n",
       " '5_H5',\n",
       " '5_D6',\n",
       " '5_H6',\n",
       " '5_D7',\n",
       " '5_H7',\n",
       " '5_D8',\n",
       " '5_H8',\n",
       " '5_D9',\n",
       " '5_H9',\n",
       " '5_D10',\n",
       " '5_H10',\n",
       " '5_D11',\n",
       " '5_H11',\n",
       " '5_D12',\n",
       " '5_H12',\n",
       " '6_A1',\n",
       " '6_E1',\n",
       " '6_A2',\n",
       " '6_E2',\n",
       " '6_A3',\n",
       " '6_E3',\n",
       " '6_A4',\n",
       " '6_E4',\n",
       " '6_A5',\n",
       " '6_E5',\n",
       " '6_A6',\n",
       " '6_E6',\n",
       " '6_A7',\n",
       " '6_E7',\n",
       " '6_A8',\n",
       " '6_E8',\n",
       " '6_A9',\n",
       " '6_E9',\n",
       " '6_A10',\n",
       " '6_E10',\n",
       " '6_A11',\n",
       " '6_E11',\n",
       " '6_A12',\n",
       " '6_E12',\n",
       " '6_B1',\n",
       " '6_F1',\n",
       " '6_B2',\n",
       " '6_F2',\n",
       " '6_B3',\n",
       " '6_F3',\n",
       " '6_B4',\n",
       " '6_F4',\n",
       " '6_B5',\n",
       " '6_F5',\n",
       " '6_B6',\n",
       " '6_F6',\n",
       " '6_B7',\n",
       " '6_F7',\n",
       " '6_B8',\n",
       " '6_F8',\n",
       " '6_B9',\n",
       " '6_F9',\n",
       " '6_B10',\n",
       " '6_F10',\n",
       " '6_B11',\n",
       " '6_F11',\n",
       " '6_B12',\n",
       " '6_F12',\n",
       " '6_C1',\n",
       " '6_G1',\n",
       " '6_C2',\n",
       " '6_G2',\n",
       " '6_C3',\n",
       " '6_G3',\n",
       " '6_C4',\n",
       " '6_G4',\n",
       " '6_C5',\n",
       " '6_G5',\n",
       " '6_C6',\n",
       " '6_G6',\n",
       " '6_C7',\n",
       " '6_G7',\n",
       " '6_C8',\n",
       " '6_G8',\n",
       " '6_C9',\n",
       " '6_G9',\n",
       " '6_C10',\n",
       " '6_G10',\n",
       " '6_C11',\n",
       " '6_G11',\n",
       " '6_C12',\n",
       " '6_G12',\n",
       " '6_D1',\n",
       " '6_H1',\n",
       " '6_D2',\n",
       " '6_H2',\n",
       " '6_D3',\n",
       " '6_H3',\n",
       " '6_D4',\n",
       " '6_H4',\n",
       " '6_D5',\n",
       " '6_H5',\n",
       " '6_D6',\n",
       " '6_H6',\n",
       " '6_D7',\n",
       " '6_H7',\n",
       " '6_D8',\n",
       " '6_H8',\n",
       " '6_D9',\n",
       " '6_H9',\n",
       " '6_D10',\n",
       " '6_H10',\n",
       " '6_D11',\n",
       " '6_H11',\n",
       " '6_D12',\n",
       " '6_H12',\n",
       " '7_A1',\n",
       " '7_E1',\n",
       " '7_A2',\n",
       " '7_E2',\n",
       " '7_A3',\n",
       " '7_E3',\n",
       " '7_A4',\n",
       " '7_E4',\n",
       " '7_A5',\n",
       " '7_E5',\n",
       " '7_A6',\n",
       " '7_E6',\n",
       " '7_A7',\n",
       " '7_E7',\n",
       " '7_A8',\n",
       " '7_E8',\n",
       " '7_A9',\n",
       " '7_E9',\n",
       " '7_A10',\n",
       " '7_E10',\n",
       " '7_A11',\n",
       " '7_E11',\n",
       " '7_A12',\n",
       " '7_E12',\n",
       " '7_B1',\n",
       " '7_F1',\n",
       " '7_B2',\n",
       " '7_F2',\n",
       " '7_B3',\n",
       " '7_F3',\n",
       " '7_B4',\n",
       " '7_F4',\n",
       " '7_B5',\n",
       " '7_F5',\n",
       " '7_B6',\n",
       " '7_F6',\n",
       " '7_B7',\n",
       " '7_F7',\n",
       " '7_B8',\n",
       " '7_F8',\n",
       " '7_B9',\n",
       " '7_F9',\n",
       " '7_B10',\n",
       " '7_F10',\n",
       " '7_B11',\n",
       " '7_F11',\n",
       " '7_B12',\n",
       " '7_F12',\n",
       " '7_C1',\n",
       " '7_G1',\n",
       " '7_C2',\n",
       " '7_G2',\n",
       " '7_C3',\n",
       " '7_G3',\n",
       " '7_C4',\n",
       " '7_G4',\n",
       " '7_C5',\n",
       " '7_G5',\n",
       " '7_C6',\n",
       " '7_G6',\n",
       " '7_C7',\n",
       " '7_G7',\n",
       " '7_C8',\n",
       " '7_G8',\n",
       " '7_C9',\n",
       " '7_G9',\n",
       " '7_C10',\n",
       " '7_G10',\n",
       " '7_C11',\n",
       " '7_G11',\n",
       " '7_C12',\n",
       " '7_G12',\n",
       " '7_D1',\n",
       " '7_H1',\n",
       " '7_D2',\n",
       " '7_H2',\n",
       " '7_D3',\n",
       " '7_H3',\n",
       " '7_D4',\n",
       " '7_H4',\n",
       " '7_D5',\n",
       " '7_H5',\n",
       " '7_D6',\n",
       " '7_H6',\n",
       " '7_D7',\n",
       " '7_H7',\n",
       " '7_D8',\n",
       " '7_H8',\n",
       " '7_D9',\n",
       " '7_H9',\n",
       " '7_D10',\n",
       " '7_H10',\n",
       " '7_D11',\n",
       " '7_H11',\n",
       " '7_D12',\n",
       " '7_H12',\n",
       " '8_A1',\n",
       " '8_E1',\n",
       " '8_A2',\n",
       " '8_E2',\n",
       " '8_A3',\n",
       " '8_E3',\n",
       " '8_A4',\n",
       " '8_E4',\n",
       " '8_A5',\n",
       " '8_E5',\n",
       " '8_A6',\n",
       " '8_E6',\n",
       " '8_A7',\n",
       " '8_E7',\n",
       " '8_A8',\n",
       " '8_E8',\n",
       " '8_A9',\n",
       " '8_E9',\n",
       " '8_A10',\n",
       " '8_E10',\n",
       " '8_A11',\n",
       " '8_E11',\n",
       " '8_A12',\n",
       " '8_E12',\n",
       " '8_B1',\n",
       " '8_F1',\n",
       " '8_B2',\n",
       " '8_F2',\n",
       " '8_B3',\n",
       " '8_F3',\n",
       " '8_B4',\n",
       " '8_F4',\n",
       " '8_B5',\n",
       " '8_F5',\n",
       " '8_B6',\n",
       " '8_F6',\n",
       " '8_B7',\n",
       " '8_F7',\n",
       " '8_B8',\n",
       " '8_F8',\n",
       " '8_B9',\n",
       " '8_F9',\n",
       " '8_B10',\n",
       " '8_F10',\n",
       " '8_B11',\n",
       " '8_F11',\n",
       " '8_B12',\n",
       " '8_F12',\n",
       " '8_C1',\n",
       " '8_G1',\n",
       " '8_C2',\n",
       " '8_G2',\n",
       " '8_C3',\n",
       " '8_G3',\n",
       " '8_C4',\n",
       " '8_G4',\n",
       " '8_C5',\n",
       " '8_G5',\n",
       " '8_C6',\n",
       " '8_G6',\n",
       " '8_C7',\n",
       " '8_G7',\n",
       " '8_C8',\n",
       " '8_G8',\n",
       " '8_C9',\n",
       " '8_G9',\n",
       " '8_C10',\n",
       " '8_G10',\n",
       " '8_C11',\n",
       " '8_G11',\n",
       " '8_C12',\n",
       " '8_G12',\n",
       " '8_D1',\n",
       " '8_H1',\n",
       " '8_D2',\n",
       " '8_H2',\n",
       " '8_D3',\n",
       " '8_H3',\n",
       " '8_D4',\n",
       " '8_H4',\n",
       " '8_D5',\n",
       " '8_H5',\n",
       " '8_D6',\n",
       " '8_H6',\n",
       " '8_D7',\n",
       " '8_H7',\n",
       " '8_D8',\n",
       " '8_H8',\n",
       " '8_D9',\n",
       " '8_H9',\n",
       " '8_D10',\n",
       " '8_H10',\n",
       " '8_D11',\n",
       " '8_H11',\n",
       " '8_D12',\n",
       " '8_H12',\n",
       " '9_A1',\n",
       " '9_E1',\n",
       " '9_A2',\n",
       " '9_E2',\n",
       " '9_A3',\n",
       " '9_E3',\n",
       " '9_A4',\n",
       " '9_E4',\n",
       " '9_A5',\n",
       " '9_E5',\n",
       " '9_A6',\n",
       " '9_E6',\n",
       " '9_A7',\n",
       " '9_E7',\n",
       " '9_A8',\n",
       " '9_E8',\n",
       " '9_A9',\n",
       " '9_E9',\n",
       " '9_A10',\n",
       " '9_E10',\n",
       " '9_A11',\n",
       " '9_E11',\n",
       " '9_A12',\n",
       " '9_E12',\n",
       " '9_B1',\n",
       " '9_F1',\n",
       " '9_B2',\n",
       " '9_F2',\n",
       " '9_B3',\n",
       " '9_F3',\n",
       " '9_B4',\n",
       " '9_F4',\n",
       " '9_B5',\n",
       " '9_F5',\n",
       " '9_B6',\n",
       " '9_F6',\n",
       " '9_B7',\n",
       " '9_F7',\n",
       " '9_B8',\n",
       " '9_F8',\n",
       " '9_B9',\n",
       " '9_F9',\n",
       " '9_B10',\n",
       " '9_F10',\n",
       " '9_B11',\n",
       " '9_F11',\n",
       " '9_B12',\n",
       " '9_F12',\n",
       " '9_C1',\n",
       " '9_G1',\n",
       " '9_C2',\n",
       " '9_G2',\n",
       " '9_C3',\n",
       " '9_G3',\n",
       " '9_C4',\n",
       " '9_G4',\n",
       " '9_C5',\n",
       " '9_G5',\n",
       " '9_C6',\n",
       " '9_G6',\n",
       " '9_C7',\n",
       " '9_G7',\n",
       " '9_C8',\n",
       " '9_G8',\n",
       " '9_C9',\n",
       " '9_G9',\n",
       " '9_C10',\n",
       " '9_G10',\n",
       " '9_C11',\n",
       " '9_G11',\n",
       " '9_C12',\n",
       " '9_G12',\n",
       " '9_D1',\n",
       " '9_H1',\n",
       " '9_D2',\n",
       " '9_H2',\n",
       " '9_D3',\n",
       " '9_H3',\n",
       " '9_D4',\n",
       " '9_H4',\n",
       " '9_D5',\n",
       " '9_H5',\n",
       " '9_D6',\n",
       " '9_H6',\n",
       " '9_D7',\n",
       " '9_H7',\n",
       " '9_D8',\n",
       " '9_H8',\n",
       " '9_D9',\n",
       " '9_H9',\n",
       " '9_D10',\n",
       " '9_H10',\n",
       " '9_D11',\n",
       " '9_H11',\n",
       " '9_D12',\n",
       " '9_H12']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w96_2blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ============================================\n",
    "# OUTPUTS\n",
    "# ============================================\n",
    "# Save dataframe as log.\n",
    "reformated_df.to_csv(f'{filename}_df.csv', index=False)\n",
    "\n",
    "# Save .xlsx for submitting the order to IDT (one sheet per plate).\n",
    "with pd.ExcelWriter(f'{filename}.xlsx') as writer:\n",
    "\n",
    "    for p in reformated_df['plate_id'].unique():\n",
    "\n",
    "        sheet_df = reformated_df[reformated_df['plate_id']==p][['Well Position', 'Name', 'Sequence']]\n",
    "\n",
    "        if args.echo: # need all wells up to P24 for IDT to understand it as a 384w plate.\n",
    "            sheet_df = sheet_df.merge(w384_df, left_on='Well Position', right_on='Well Position', how='right')\n",
    "\n",
    "        sheet_df.to_excel(writer, sheet_name=f'{date}_{args.order_name}.{int(p)}_{args.species}_{enzyme}', index=False)\n",
    "\n",
    "# Save FASTA file(s) of expression products for protparam.\n",
    "for gg_v in args.gg_vector:\n",
    "    with open(f'{filename}_into_{gg_v}.fa', 'w') as f:\n",
    "        for i, r in reformated_df.iterrows():\n",
    "            if 'y' in r['design_id']:\n",
    "                pass\n",
    "\n",
    "            else:\n",
    "                if 'x' in r['design_id']:\n",
    "                    f.write('>' + r['design_id'].replace('x', '') + '__' + '__'.join(r['Name'].split('__')[1:]) + '\\n')\n",
    "                    f.write(r[f'exp_aa_seq_from_{gg_v}'] + '\\n')\n",
    "\n",
    "                else:\n",
    "                    f.write('>' + r['Name'] +  '\\n')\n",
    "                    f.write(r[f'exp_aa_seq_from_{gg_v}'] + '\\n')\n",
    "\n",
    "\n",
    "# Save text file with plate layouts and cloning instructions.\n",
    "txt = f'CLONING INSTRUCTIONS FOR {date}_{args.order_name}_{args.species}_{enzyme}:\\n'\n",
    "single_frag = reformated_df[reformated_df['is_frag']==False]\n",
    "dual_frag = reformated_df[reformated_df['is_frag']==True]\n",
    "\n",
    "if len(single_frag) > 0:\n",
    "    txt += f' * Number of 1-fragment reactions: {len(single_frag)}\\n'\n",
    "\n",
    "if len(dual_frag) > 0:\n",
    "    txt += f' * Number of 2-fragments reactions: {int(len(dual_frag)/2)}\\n'\n",
    "\n",
    "txt += '\\n'\n",
    "\n",
    "# Layouts.\n",
    "w96 = np.reshape([r + str(c) for r in 'ABCDEFGH' for c in range(1,13)], (8, 12))\n",
    "idx2row = {i:r for i, r in enumerate('ABCDEFGH')}\n",
    "for p in reformated_df['plate_id'].unique():\n",
    "    plate_df = reformated_df[reformated_df['plate_id']==p]\n",
    "    txt += f'{date}_{args.order_name}.{int(p)}_{args.species}_{enzyme}\\n'\n",
    "    txt += '  ' + ''.join([str(c).center(11) for c in np.arange(1,13)]) + '\\n'\n",
    "    for i, row in enumerate(w96):\n",
    "        txt += idx2row[i] + ' |'\n",
    "\n",
    "        for j, well in enumerate(row):\n",
    "\n",
    "            if well in plate_df['Well Position'].values:\n",
    "                well_content = plate_df[plate_df['Well Position']==well]['design_id'].values[0].center(10)\n",
    "\n",
    "            else:\n",
    "                well_content = ''.center(10)\n",
    "\n",
    "            txt += well_content + '|'\n",
    "        txt += '\\n'\n",
    "    txt += '\\n\\n'\n",
    "\n",
    "with open(f'{filename}_cloning_instructions.txt', 'w') as f:\n",
    "    f.write(txt)\n",
    "    \n",
    "    \n",
    "# Unless supressed, generate cloned plasmid maps.\n",
    "if not args.no_plasmids:\n",
    "    os.makedirs(output_folder + 'cloned_plasmids/', exist_ok=True) # make a subfolder to store the plasmid maps\n",
    "\n",
    "    for gg_v in args.gg_vector:\n",
    "        \n",
    "        for i, r in reformated_df.iterrows():\n",
    "            if 'y' in r['design_id']:\n",
    "                pass\n",
    "        \n",
    "            else:\n",
    "                plasmid_name = r['design_id'].replace('x', '') + '_into_' + gg_v\n",
    "                \n",
    "                with open(output_folder + 'cloned_plasmids/' + plasmid_name + '.fa' , 'w') as f:\n",
    "                    f.write('>' + plasmid_name + '\\n')\n",
    "                    f.write(r[f'{gg_v}_cloned_plasmid_seq'] + '\\n')\n",
    "\n",
    "\n",
    "# Make symlink of PDBs with design_id + well + plate appended to the filename.\n",
    "if args.order_pdbs != None:\n",
    "    print('Making PDB symlinks...')\n",
    "\n",
    "    for i, r in reformated_df.iterrows():\n",
    "\n",
    "        if 'y' in r['design_id']: # don't make symlinks for y-fragments\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            info_label = '__'.join(r['Name'].split('__')[:3]).replace('x', '')\n",
    "\n",
    "            if '_isheterooligomer' in r['design_name']:\n",
    "                pdb_name = '_'.join(r['design_name'].split('_')[:-2]) + '.pdb'\n",
    "\n",
    "            else:\n",
    "                pdb_name = r['design_name'] + '.pdb'\n",
    "\n",
    "            if args.order_pdbs + pdb_name in glob.glob(f'{args.order_pdbs}*.pdb'):\n",
    "                os.system(f'ln -s {pdb_name} {args.order_pdbs}{info_label}__{pdb_name}') # create symlink of PDB with protein ID, well ID and plate ID\n",
    "\n",
    "if len(skipped) > 0:\n",
    "    print(f'The following {len(skipped)} designs could not be converted to eBlocks and were skipped:')\n",
    "    with open(f'{filename}_FAILED.fa', 'w') as f:\n",
    "        for k, v in skipped.items():\n",
    "            f.write(f'>{k}\\n{v}\\n')\n",
    "            print(f'>{k}\\n{v}')\n",
    "\n",
    "else:\n",
    "    print('All designs were successfully converted to eBlocks.')\n",
    "\n",
    "print(f\"Order contains {len(reformated_df)} eBlocks across {len(reformated_df['plate_id'].unique())} plate(s).\")\n",
    "print(f\"Place order at https://www.idtdna.com/site/order/plate/eblocks\\n\")\n",
    "\n",
    "if args.verbose:\n",
    "    print(\n",
    "    \"John Bercow says:\\n\"\n",
    "    \"////////////////////////////////////////////+///////////////////////////////////////////////////////\\n\"\n",
    "    \"////////////////////////////////////+shdmNNNNmmhs+://///////////////////////////////////////////////\\n\"\n",
    "    \"/////////////////////////////////+ymNNMMMMMNNhssso+so+//////////////////////////////////////////////\\n\"\n",
    "    \"///////////////////////////////ohmNNNNMMMMMNhs++::++sso+////////////////////////////////////////////\\n\"\n",
    "    \"////////////////////////////+ymNNMNNNMMMMMNmhs+///:::+os+///////////////////////////////////////////\\n\"\n",
    "    \"//////////////////////////+ymNMMMNMMMMMMMNmhyo//::/++yysy///////////////////////////////////////////\\n\"\n",
    "    \"/////////////////////////shmMMMMMMMMMMMMMNdhyyoosshhhddhy+://///////////////////////////////////////\\n\"\n",
    "    \"/////////////////////////+hmNNNNNMMMNmdmddhyyyydNdNNNhs+/--/////////////////////////////////////////\\n\"\n",
    "    \"///////////////////////////oyhddhdddhhysyyyhhdNMMMMNmho:-../////////////////////////////////////////\\n\"\n",
    "    \"//////////////////////////:+yssyyyddmNNNmNmmmmNMMMMmhhys+..:////////////////////////////////////////\\n\"\n",
    "    \"///////////////////////////shhhhddmmNNNNNNNNNNmmNNNmhyys+:-/////////////////////////////////////////\\n\"\n",
    "    \"//////////////////////////+hdNddmmNNNNNNNNNNmmhysso+osys+:-/////////////////////////////////////////\\n\"\n",
    "    \"/////////////////////////:`./hmNNNNNNNNNNNmmds/:-----:/o-:://///////////////////////////////////////\\n\"\n",
    "    \"//////////////////////////.``.os//oymNNmNmmmdy:.``-:-/o+`.://///////////////////////////////////////\\n\"\n",
    "    \"//////////////////////////-.-`+h+` `-+oyyydmdy:..++:+/yy..://///////////////////////////////////////\\n\"\n",
    "    \"//////////////////////////:::/NNds+:-.:+odmdhysoooyshdd:`-//////////////////////////////////////////\\n\"\n",
    "    \"//////////////////////////:/sdMNmhyhhhyhhdhhhdhyooydmd/: `--:///////////////////////////////////////\\n\"\n",
    "    \"///////////////////////////-+dmmmdosyyysssyhhhy+://::..y+    `.--://////////////////////////////////\\n\"\n",
    "    \"/////////////////////////::..----:yyyyssooooo+/---:.``/NN`        ``..--:///////////////////////////\\n\"\n",
    "    \"//////////////////////:-.`   .:/sdmhhhys+/::-........-dMM:              ``.:////////////////////////\\n\"\n",
    "    \"//////////////////::-``       ``.-++syso+/-.....--..-hMMM-                 ``.-::///////////////////\\n\"\n",
    "    \"///////////////::.`           .`  ./o+oo+/---:::..-+dMMMN`                      `.-:////////////////\\n\"\n",
    "    \"/////////////:-`              :/-:+/+////::---.`-+dNMMMMs                          `:///////////////\\n\"\n",
    "    \"///////////:.`                `-:-``.-::-..`   `-+osydmm-                           `-//////////////\\n\"\n",
    "    \"/////////:-                        `...````   .---``+yyo                              -/////////////\\n\"\n",
    "    \"////////:`                               ..`  .----sNMMo                               -////////////\\n\"\n",
    "    \"///////:                                `+o/:.+y`:hMMMM-                                :///////////\\n\"\n",
    "    \"///////.                                 -mms/`/+yMMMMm                                 `://////////\\n\"\n",
    "    \"//////:                                   +Mm:./h/dMMM+                                  `://///////\\n\"\n",
    "    \"/////:`                                    om.h:://hNN.                                   `:////////\\n\"\n",
    "    \"/////.                                      .:--:ho+ho                                     `:///////\\n\"\n",
    "    \"////:                                        ..//s+/s`                                      .///////\\n\"\n",
    "    \"////`                                         `o+:///                                        -//////\\n\"\n",
    "    \"///:                                           -::sh`                                        `://///\\n\"\n",
    "    \"///.                                            :o+:                                          -/////\\n\"\n",
    "    \"//:`                                            -s/`                                           -////\\n\"\n",
    "    \"//-                                              ``                                            `:///\\n\"\n",
    "    \"//-                                                                                             -///\\n\"\n",
    "    \"//.                                                                                             `://\\n\"\n",
    "    \"/:`                                             ss/.`                                            `:/\\n\"\n",
    "    \"/-                                             `NMMMmy+:`                                         ./\\n\"\n",
    "    \"/.                          ``                  oMMMMMMMNdy/.          ``                          :\\n\"\n",
    "    \":`                           +:                  hMMMMMMMMMMMmy+-`  `+dNMms/.                      .\\n\"\n",
    "    \"-                            .d:-/+oys           .NMMMMMMMMMMMMMMMdshNMMMMMMmh+.                   `\\n\"\n",
    "    \"`                        `:osshdNNNh/.            /MMMMMMMMMMMMMMMMMMMMMNNMNmNNm+`                  \\n\"\n",
    "    \"                        -sNNNmNmNNmho+-            sMMMMMMMMMMMMMMMMMMMMmhmmNNNNN+                  \\n\"\n",
    "    \"                       .++hhyo+::::/.``            `dMMMMMMMMMMMMMMMMMMmshhddyNNd-                  \\n\"\n",
    "    \"``````.....````````....-:+hhhys+..+y-`...........```:dddddmmdmmddddhyhds-:/soydy/.......``````......\\n\"\n",
    "    \"://+ydmNNNmdyo///:/mmmmmmmNNNNmmmho//:dmmmmmmmmmmmdyo/////mNNNNNNNNNmNNmy/+mmNNmmmmmmmmmdy+///mmmmmy\\n\"\n",
    "    \"/+dNMMMMMMMMMMmo///MMMMMMMMMMMMMMMMy//NMMMMMMMMMMMMMMms///MMMMMMMMMMMMMMh/oMMMMMMMMMMMMMMMNs//MMMMMh\\n\"\n",
    "    \"+NMMMMMNmmNMMMMNs//MMMMMdsssssmMMMMM//NMMMMmsssyhmMMMMMs//MMMMMdsssssssso/oMMMMMhsssssNMMMMm//NMMMMh\\n\"\n",
    "    \"dMMMMNs///omMMMMN//MMMMMNdddddNMMMMm//NMMMMd//////dMMMMN//MMMMMNddddddddy/oMMMMMmdddddNMMMMd//NMMMMy\\n\"\n",
    "    \"NMMMMd/////yMMMMM+/MMMMMMMMMMMMMMNd+//NMMMMd//////yMMMMM+/MMMMMMMMMMMMMMh/oMMMMMMMMMMMMMMNh///dMMMMo\\n\"\n",
    "    \"hMMMMMy+/+sNMMMMm//MMMMMmyymMMMMNo////NMMMMd/////oNMMMMm//MMMMMdsssssssss/oMMMMMdyhNMMMMN+////sNNNM/\\n\"\n",
    "    \"/mMMMMMMNMMMMMMNo//MMMMMy///mMMMMN+///NMMMMNhhhhmMMMMMN+//MMMMMdsssssssss/oMMMMMs//+mMMMMm+/////////\\n\"\n",
    "    \"//yNMMMMMMMMMMh+///MMMMMy////dMMMMNo//NMMMMMMMMMMMMMNy////MMMMMMMMMMMMMMh/oMMMMMs///+mMMMMN+//MMMMMh\\n\"\n",
    "    \"////oydmmmdhs//////ddddds/////hddddd+/hddddddddddhyo//////ddddddddddddddy/+dddddo/////hddddh//ddddds\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "john_bercow",
   "language": "python",
   "name": "john_bercow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
